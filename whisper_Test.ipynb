{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whikqp/utils/blob/main/whisper_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 語音轉文字 AI 工具\n",
        "本工具使用 [OpenAI 的開源工具 Whisper](https://github.com/openai/whisper) 模型, 可以相對精準的將隨語音轉文字。\n",
        "\n",
        "# (一) 選擇適合的運作環境： T4 GPU\n",
        "本 Colab 虛擬機器使用為免費、多GPU的環境。已指定 T4 GPU 版本。\n",
        "\n",
        "若由  Github 直接開啟，可以忽略此說明。"
      ],
      "metadata": {
        "id": "Z8j9agRoP2Ef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ESQe_Qm7Ceoz",
        "outputId": "6cd37a60-4281-4108-f78f-845401321103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-wot06ca3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-wot06ca3\n",
            "  Resolved https://github.com/openai/whisper.git to commit 517a43ecd132a2089d85f4ebc044728a71d49f6e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.6.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.0.2)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m123.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803708 sha256=fca3642b2f1408685d92382b737ebd10fc4f792508be6b6ffdd1355e6a112784\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l0o6i6mp/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# @title (1) 安裝 whisper\n",
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) 掛載雲端硬碟\n",
        "1. 透過 Coloab 左邊的操作介面掛載\n",
        "2. 上傳音檔/影像檔到 Google drive\n",
        "  - 個人偏好在 Google drive 建一個 tmp 資料夾\n",
        "  - 將音檔上傳到 tmp 資料夾\n",
        "  - 在 Colab 左邊的掛載介面找到 drive => MyDrive => tmp\n",
        "  - 點選上載的音檔，按滑鼠右鍵，點選 複製路徑\n",
        "3. 將複製的路徑貼到轉檔區塊的 filenames 欄位中\n"
      ],
      "metadata": {
        "id": "vj1rk1zOKoh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title (3) 轉檔\n",
        "import os\n",
        "filename = \"/content/424.MP3\" # @param {type:\"string\"}\n",
        "#@markdown 設定使用的模型, 請參考 [Whisper Model Card](https://github.com/openai/whisper/blob/main/model-card.md) 選擇適合的模型\n",
        "model= \"large-v3\" # @param {type:\"string\"}\n",
        "\n",
        "#@markdown 設定主要的語言，如 Chinese, English，其它請參表 [tokenizer 文件](https://github.com/openai/whisper/blob/main/whisper/tokenizer.py)\n",
        "language = \"Chinese\" # @param {type:\"string\"}\n",
        "os.chdir(os.path.dirname(filename))\n",
        "os.getcwd()\n",
        "!whisper \"{filename}\" --model {model} --language {language}"
      ],
      "metadata": {
        "id": "t1k2RIWHDfhz",
        "outputId": "94f2da76-5798-4036-a95e-80651d1cc9b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:09.900] 度电成本的降低是全球光伏产业的共同追求\n",
            "[00:09.900 --> 00:12.720] 从材料科学到制造工艺\n",
            "[00:12.720 --> 00:15.900] 从储能技术到智能管理\n",
            "[00:15.900 --> 00:21.320] 每一次技术的通过都在推动着光电转化性率的提升\n",
            "[00:21.320 --> 00:27.060] 中国光伏产业正以科技之力为全球能源转型\n",
            "[00:27.060 --> 00:30.860] 奉献着自己的智慧与力量\n",
            "[00:30.860 --> 00:39.620] 四川省甘孜州\n",
            "[00:39.620 --> 00:41.840] 扎拉多桑雪山之巅\n",
            "[00:41.840 --> 00:45.060] 世界第三级的边缘之地\n",
            "[00:45.060 --> 00:49.680] 群生环绕的亚龙江\n",
            "[00:49.680 --> 00:52.780] 自八眼喀拉山的楼门腾而下\n",
            "[00:52.780 --> 00:55.720] 3830米的天然落差\n",
            "[00:55.720 --> 00:57.040] 跌宕着彭博\n",
            "[00:57.060 --> 01:09.680] 亚龙江流域水分沟硬体化基地给出了答案\n",
            "[01:09.680 --> 01:15.400] 这里是全球一件最大海拔最高的水光互补电站\n",
            "[01:15.400 --> 01:21.120] 水光和谐共舞的背后是源于对生态的观察和联系\n",
            "[01:21.120 --> 01:26.200] 克拉利西光伏电站所属的甘孜州亚龙江流域\n",
            "[01:26.200 --> 01:27.040] 具有天然光和天然光的互补电站\n",
            "[01:27.060 --> 01:28.160] 天然的水光互补优势\n",
            "[01:28.160 --> 01:30.940] 光伏发电呈现冬春季处理大\n",
            "[01:30.940 --> 01:32.680] 夏秋季处理小的世界特点\n",
            "[01:32.680 --> 01:35.200] 与水利发电夏季风水时\n",
            "[01:35.200 --> 01:38.200] 冬季风水时的特点形成天然的年代互补\n",
            "[01:38.200 --> 01:42.980] 克拉利西光伏电站通过与四川省内最大的多年条件的电站\n",
            "[01:42.980 --> 01:44.980] 两河口水电站互补开发\n",
            "[01:44.980 --> 01:48.600] 首次实现了百万千万计的水光互补\n",
            "[01:48.600 --> 01:51.080] 实现了发电综合效率最大化\n",
            "[01:51.080 --> 01:55.800] 在海拔4500米以上无人之地追求效率\n",
            "[01:55.800 --> 01:57.040] 似乎是一件很重要的事情\n",
            "[01:57.060 --> 01:58.820] 想强人所难的任务\n",
            "[01:58.820 --> 02:00.880] 敢拖一层皮再造一条江\n",
            "[02:00.880 --> 02:03.700] 克拉光伏的工地上有这样一条标语\n",
            "[02:03.700 --> 02:06.180] 因为天晴的时候如果没有防护措施\n",
            "[02:06.180 --> 02:08.680] 在工地待一个小时就会再拖一层皮\n",
            "[02:08.680 --> 02:10.960] 除了强烈的紫白线\n",
            "[02:10.960 --> 02:14.500] 这里的含氧量也只有平原地区的一半\n",
            "[02:14.500 --> 02:17.840] 一举一动都需要花费更多的力气\n",
            "[02:17.840 --> 02:21.360] 建设的每一步都必须做好整体工法\n",
            "[02:21.360 --> 02:23.940] 才能不辜负每一份付出\n",
            "[02:23.940 --> 02:26.780] 山河险峻 物力维艰\n",
            "[02:27.060 --> 02:32.180] 亚龙江流域建设 捐成科技智慧的新竹\n",
            "[02:32.180 --> 02:33.900] 2023年9月12日\n",
            "[02:33.900 --> 02:37.700] 亚龙江流域与华为公司成立了联合创新中心\n",
            "[02:37.700 --> 02:40.060] 以解决亚龙江流域水空工一体化\n",
            "[02:40.060 --> 02:43.060] 四贯基地关键技术难题为导向\n",
            "[02:43.060 --> 02:47.720] 发挥华为公司在数字化 智能化 转型的技术优势\n",
            "[02:47.720 --> 02:50.160] 深入研究光伏电站规划建设\n",
            "[02:50.160 --> 02:54.440] 生产运行阶段的科技创新和数字化新技术运用\n",
            "[02:54.440 --> 02:58.120] 从规划 建设 维护到运营\n",
            "[02:58.120 --> 03:02.800] 全生命中期数字化解决方案应运而生\n",
            "[03:02.800 --> 03:04.200] 在设计阶段\n",
            "[03:04.200 --> 03:06.160] 通过无人机实景建模\n",
            "[03:06.160 --> 03:09.600] 可为设计人员提供可视化3D模型\n",
            "[03:09.600 --> 03:15.180] 来弥补偏远地区数据资源积累少的问题\n",
            "[03:15.180 --> 03:18.640] 在面积1600万平方米的厂站中\n",
            "[03:18.640 --> 03:23.320] 运维人员肩负着200多万块光伏组件的为减重任\n",
            "[03:23.320 --> 03:24.280] 是一项极为\n",
            "[03:24.280 --> 03:26.420] 艰巨的任务\n",
            "[03:26.420 --> 03:31.640] 现场应用华为智能IV诊断与无人机AI图像识别技术\n",
            "[03:31.640 --> 03:37.840] 联动交叉诊断能更快更准确地定位组件及故障问题\n",
            "[03:37.840 --> 03:39.940] 智慧不止于此\n",
            "[03:39.940 --> 03:44.260] 亚龙将携手华为首创的智能运维驾驶舱系统\n",
            "[03:44.260 --> 03:47.500] 将高原天衔变得尽在掌握\n",
            "[03:47.500 --> 03:52.080] 甚至让繁重的运维工作有了抑制游戏的趣味\n",
            "[03:52.080 --> 03:53.100] 自然运维驾驶舱\n",
            "[03:53.100 --> 03:53.680] 这套系统\n",
            "[03:53.680 --> 03:53.980] 有时候\n",
            "[03:53.980 --> 03:55.000] 就像玩游戏一样\n",
            "[03:55.000 --> 03:58.180] 在这张数字软身而成的地图上排兵布阵\n",
            "[03:58.180 --> 04:02.560] 通过智能导航可缩短负障设备定位时间30%以上\n",
            "[04:02.560 --> 04:06.640] 大幅缩减运维人员靠走路来寻找负障设备的距离\n",
            "[04:06.640 --> 04:10.420] 降低我们在超高海拔地区作为的强度和安全风险\n",
            "[04:10.420 --> 04:13.360] 使整体运维工作效率提升5%以上\n",
            "[04:14.800 --> 04:18.520] 全生命周期数字化解决方案助力之下\n",
            "[04:18.520 --> 04:21.700] 工程效率得到显著提升\n",
            "[04:21.700 --> 04:22.300] 然而\n",
            "[04:22.300 --> 04:23.820] 亚龙将并不满足\n",
            "[04:23.820 --> 04:27.240] 与此对更高能源利用率的渴望\n",
            "[04:27.240 --> 04:30.080] 鞭策着科技的不断进步\n",
            "[04:30.080 --> 04:31.160] 在运营阶段\n",
            "[04:31.160 --> 04:34.500] 根据过年来在实证电站机里的数百种指标\n",
            "[04:34.500 --> 04:37.200] 我们构建了电站发电效率评估系统\n",
            "[04:37.200 --> 04:41.160] 借助正好系统可以清晰地看到影响发电量的瓶颈\n",
            "[04:41.160 --> 04:45.900] 同而对症下药解决疑难导致实现与问题和数据驱动误会\n",
            "[04:45.900 --> 04:46.920] 提升发电效率\n",
            "[04:48.180 --> 04:52.500] 未来亚龙将流域水风光一体化基地\n",
            "[04:52.500 --> 04:53.520] 将通过光\n",
            "[04:53.820 --> 04:56.280] 风水储备合力\n",
            "[04:56.280 --> 04:59.640] 去年均发电量达到约两千亿金瓦时\n",
            "[04:59.640 --> 05:06.600] 亚龙将公司携手华为首次在业内提出全生命周期数字化解决方案\n",
            "[05:06.600 --> 05:09.600] 为新能源电站注入AI动力\n",
            "[05:09.600 --> 05:12.480] 全面提升规件为影效率\n",
            "[05:12.480 --> 05:16.080] 推动新能源电站走向自动驾驶\n",
            "[05:16.080 --> 05:19.320] 高建清洁能源大基地的全球标杆\n",
            "[05:20.520 --> 05:23.460] 在亚龙键盘自然与科技\n",
            "[05:23.820 --> 05:25.860] 与工人\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (二) 取得法說會文字\n",
        "## 1. 請先執行 (1) 安裝 whisper\n",
        "## 2. 再執行 (4) 法說會逐字稿, ...."
      ],
      "metadata": {
        "id": "OjboOjHQ6qOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title (4) 法說會逐字稿，當影片在 Youtube 可直接使用這個\n",
        "!pip install yt-dlp\n",
        "\n",
        "tubeUrl = \"https://www.youtube.com/watch?v=Q6sI_eY6sdU\" # @param {type:\"string\"}\n",
        "import os\n",
        "from yt_dlp import YoutubeDL\n",
        "companyName=\"科技小電報\" # @param {type:\"string\"}\n",
        "model= \"large\" # @param {type:\"string\"}\n",
        "language = \"Chinese\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "filename = companyName+\".m4a\"\n",
        "ydl_opts = {'overwrites': True, 'format': 'bestaudio[ext=m4a]', 'outtmpl': filename}\n",
        "with YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([tubeUrl])\n",
        "\n",
        "!whisper \"{filename}\" --model {model} --language {language}\n",
        "\n",
        "from google.colab import files\n",
        "exts=[\"txt\",\"srt\",\"tsv\",\"vtt\"]\n",
        "for ext in exts:\n",
        "  files.download('{}.{}'.format(companyName,ext))"
      ],
      "metadata": {
        "id": "2kNMQIdNgS48",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}